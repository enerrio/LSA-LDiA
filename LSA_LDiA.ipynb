{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Semantic Analysis & Latent Dirichlet Allocation\n",
    "---\n",
    "Perform Latent Semantic Analysis (AKA latent semantic indexing) & Latent Dirichlet Allocation (LDiA) on a spam dataset and compare results.\n",
    "\n",
    "> You shall know a word by the company it keeps\n",
    "> \n",
    "> \\- J.R. Firth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas version: 0.22.0\n",
      "Numpy version: 1.12.1\n",
      "Seaborn version: 0.8.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from nltk.tokenize.casual import casual_tokenize\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDiA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "%matplotlib inline\n",
    "\n",
    "print('Pandas version:', pd.__version__)\n",
    "print('Numpy version:', np.__version__)\n",
    "print('Seaborn version:', sns.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (4837, 2)\n",
      "Number of spam emails: 638\n",
      "13.19% of emails are spam\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spam</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   spam                                               text\n",
       "0     0  Go until jurong point, crazy.. Available only ...\n",
       "1     0                      Ok lar... Joking wif u oni...\n",
       "2     1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3     0  U dun say so early hor... U c already then say...\n",
       "4     0  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "sms = pd.read_csv('sms-spam.csv', index_col=0)\n",
    "\n",
    "print(\"Data shape:\", sms.shape)\n",
    "print('Number of spam emails:', sms['spam'].sum())\n",
    "print('{:.2f}% of emails are spam'.format(100. * (sms['spam'].sum() / len(sms))))\n",
    "\n",
    "sms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam  0 : Shall i get my pouch?\n",
      "Spam  0 : Also maaaan are you missing out\n",
      "Spam  1 : Jamster! To get your free wallpaper text HEART to 88888 now! T&C apply. 16 only. Need Help? Call 08701213186.\n",
      "Spam  0 : I would but I'm still cozy. And exhausted from last night.nobody went to school or work. Everything is closed.\n",
      "Spam  0 : I'm eatin now lor, but goin back to work soon... E mountain deer show huh... I watch b4 liao, very nice...\n"
     ]
    }
   ],
   "source": [
    "# Print some sample text\n",
    "for idx, sample in sms.sample(5).iterrows():\n",
    "    print(\"Spam \", sample['spam'], \":\", sample['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10f9579e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot class distribution\n",
    "sns.barplot(x=['not spam', 'spam'], y=[len(sms) - sms['spam'].sum(), sms['spam'].sum()])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize Text\n",
    "---\n",
    "Use TfidfVectorizer to create bag of words (occurance counts) and then reweight counts with inverse document frequency values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(4837, 9232)\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(tokenizer=casual_tokenize)\n",
    "tfidf_docs = tfidf.fit_transform(raw_documents=sms['text']).toarray()\n",
    "\n",
    "print(type(tfidf_docs))\n",
    "print(tfidf_docs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Size of our vocabulary is 9,232 words or tokens. Now we have a **term-document matrix** (AKA **context matrix**). Each row is a document and each column is a term in our vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform LSA\n",
    "---\n",
    "Use sci-kit learn's PCA (principal component analysis) to perform latent semantic analysis. PCA uses singular value decomposition (SVD) to project high dimensional data onto a lower dimensional space. SVD produces 3 matrices (U, S, V) that multiplied together, give the original input matrix. SVD groups terms that have a high correlation and we can think of them as \"topic\" vectors.\n",
    "\n",
    "* U: Left singular vectors\n",
    "* S: Principal components (topics)\n",
    "* V: Right singular vectors\n",
    "\n",
    "The middle matrix is a diagonal matrix of singular values (AKA eigenvalues) and is the variance of each topic. In non-NLP problems it's known as the principal components of the input data. By keeping the principal components with high values we keep the dimensions that \"explain\" the original data. In NLP-speak this means that principal components with high values are our topics which are used a lot throughout our original corpus.\n",
    "\n",
    "By truncating the size of SVD's output matrices, we can reduce the dimensionality. PCA does this for us when we set `n_components` as the number of topics we want (i.e. our new dimensionality)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of topics: 16\n",
      "Shape of topic_vectors: (4837, 16)\n"
     ]
    }
   ],
   "source": [
    "# Use PCA to reduce dimensions\n",
    "num_topics = 16\n",
    "pca = PCA(n_components=num_topics)\n",
    "pca = pca.fit(tfidf_docs)\n",
    "topic_vectors = pca.transform(tfidf_docs)\n",
    "\n",
    "print('Number of topics:', num_topics)\n",
    "print('Shape of topic_vectors:', topic_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1100521d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot explained variance for each topic\n",
    "sns.barplot(x=[i for i in range(1, pca.n_components_+1)], y=pca.explained_variance_ratio_)\n",
    "plt.xlabel('Topics')\n",
    "plt.ylabel('Percentage of variance explained')\n",
    "plt.title('Explained Variance Ratio')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity\n",
    "---\n",
    "Here we can use the cosine similarity metric to measure the simliarity between two topic vectors. In this case we take the first 10 documents and compare their topic vectors against itself and the other 9 topic vectors. The cosine similarity is defined as:\n",
    "\n",
    "K = **A** dot **B** / norm(A) \\* norm(B)\n",
    "\n",
    "When compared against itself, the similarity score should be 1 which indicates identical normalized vectors that point in the same direction along all dimensions. A similarity score closer to 1 means the two documents are similar to each other. A score of 0 means the two vectors are orthogonal to each other in all dimensions and are using different terms. A score closer to -1 means that are \"opposite\" of each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>not spam</th>\n",
       "      <th>not spam</th>\n",
       "      <th>spam</th>\n",
       "      <th>not spam</th>\n",
       "      <th>not spam</th>\n",
       "      <th>spam</th>\n",
       "      <th>not spam</th>\n",
       "      <th>not spam</th>\n",
       "      <th>spam</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>not spam</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not spam</th>\n",
       "      <td>0.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not spam</th>\n",
       "      <td>0.7</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not spam</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>-0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not spam</th>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not spam</th>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          not spam  not spam  spam  not spam  not spam  spam  not spam  \\\n",
       "not spam       1.0       0.6  -0.1       0.7       0.0  -0.3      -0.2   \n",
       "not spam       0.6       1.0  -0.1       0.8      -0.2   0.0      -0.2   \n",
       "spam          -0.1      -0.1   1.0      -0.1       0.2   0.4       0.1   \n",
       "not spam       0.7       0.8  -0.1       1.0      -0.2  -0.3      -0.1   \n",
       "not spam       0.0      -0.2   0.2      -0.2       1.0   0.2      -0.1   \n",
       "spam          -0.3       0.0   0.4      -0.3       0.2   1.0      -0.2   \n",
       "not spam      -0.2      -0.2   0.1      -0.1      -0.1  -0.2       1.0   \n",
       "not spam      -0.2      -0.2   0.3      -0.3       0.1   0.1       0.2   \n",
       "spam          -0.3      -0.2   0.5      -0.2      -0.3   0.3      -0.2   \n",
       "spam          -0.3      -0.1   0.4      -0.1      -0.2   0.4      -0.1   \n",
       "\n",
       "          not spam  spam  spam  \n",
       "not spam      -0.2  -0.3  -0.3  \n",
       "not spam      -0.2  -0.2  -0.1  \n",
       "spam           0.3   0.5   0.4  \n",
       "not spam      -0.3  -0.2  -0.1  \n",
       "not spam       0.1  -0.3  -0.2  \n",
       "spam           0.1   0.3   0.4  \n",
       "not spam       0.2  -0.2  -0.1  \n",
       "not spam       1.0   0.1   0.5  \n",
       "spam           0.1   1.0   0.3  \n",
       "spam           0.5   0.3   1.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "index = ['spam' if x == 1 else 'not spam' for x in sms['spam'].iloc[:10]]\n",
    "similar_df = pd.DataFrame(cosine_similarity(topic_vectors[:10], topic_vectors[:10]).round(1), index=index, columns=index)\n",
    "similar_df\n",
    "\n",
    "# Exactly the same as above\n",
    "# topic_vectors = (topic_vectors.T / np.linalg.norm(topic_vectors, axis=1)).T\n",
    "# topic_vectors[:10].dot(topic_vectors[:10].T).round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first row we can see that the cosine similarity for the first element is 1 because it is being compared against itself. It also gets high similarity scores for other \"not spam\" vectors (with some exceptions). We are able to show using cosine similarity that \"not spam\" vectors are similar to other \"not spam\" vectors and vice versa for \"spam\" vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building & Evaluating a Classifier\n",
    "---\n",
    "Now we will use a linear discriminant analysis (LDA) classifier to try and classify our text samples as spam or ham."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (3385, 16)\n",
      "X_test shape: (1452, 16)\n",
      "y_train shape: (3385,)\n",
      "y_test shape: (1452,)\n"
     ]
    }
   ],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(topic_vectors, sms['spam'], test_size=0.3, random_state=21)\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "              solver='svd', store_covariance=False, tol=0.0001)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create LDA model and fit to training data\n",
    "clf = LDA()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 96.07%\n"
     ]
    }
   ],
   "source": [
    "score = clf.score(X_test, y_test)\n",
    "\n",
    "print(\"Test accuracy: {:.2f}%\".format(100. * score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Custom Emails\n",
    "---\n",
    "Let's create dummy emails and see if it can correctly classify them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   true  preds                                              email\n",
      "0     0      0  Hi John, this is Juan. Just wanted to let you ...\n",
      "1     1      0  Earn $1000 a day. Work from home and make mone...\n",
      "2     1      1  Earn £1000 a day. Work from home and make mone...\n",
      "3     0      0  I'm going to be late, please order dinner toni...\n",
      "4     1      1  Free money!! Join our network of professionls ...\n"
     ]
    }
   ],
   "source": [
    "emails = [\"Hi John, this is Juan. Just wanted to let you know our meeting was rescheduled for next week. Will let you know more later. - Juan\",\n",
    "         \"Earn $1000 a day. Work from home and make money with me. Reply to make money now!\",\n",
    "         \"Earn £1000 a day. Work from home and make money with me. Reply to make money now!\",\n",
    "         \"I'm going to be late, please order dinner tonight for everyone.\",\n",
    "         \"Free money!! Join our network of professionls and get rewarded!\"]\n",
    "true_labels = [0, 1, 1, 0, 1]\n",
    "\n",
    "emails_tfidf = tfidf.transform(emails).toarray()\n",
    "emails_topic = pca.transform(emails_tfidf)\n",
    "preds = clf.predict(emails_topic)\n",
    "test_df = pd.DataFrame()\n",
    "test_df['true'] = true_labels\n",
    "test_df['preds'] = preds\n",
    "test_df['email'] = emails\n",
    "print(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first email was correctly classified as \"not spam\" but the next 2 samples are supposed to be marked as \"spam.\" The second and third emails are exactly the same with one difference: the dollar sign is replaced with a pound sign. As a result, the second email is incorrectly classified as \"not spam\" and the third email is correctly classified as \"spam.\" This is a useful demonstration of the bias that exists in the dataset. The dataset we are using was collected from a UK forum about spam and non-spam emails and in the dataset there are email samples that, when referring to money, use the British pound sign. So it makes sense that our topic vectors do not understand the difference (and similarity) between the British pound and the American dollar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation (LDiA)\n",
    "---\n",
    "Now we will use Latent Dirichlet Allocation algorithm to classify text as either spam or not spam. The main difference between LSA and LDiA is that LDiA treats documents as a mix of topics. So one text sample can be 40% Topic A and 60% Topic B.\n",
    "\n",
    "LDiA also tends to work better with raw bag-of-words counts than normalized TF-IDF vectors, so we will use CountVectorizer to build vectors of term occurances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>!</th>\n",
       "      <th>\"</th>\n",
       "      <th>#</th>\n",
       "      <th>#150</th>\n",
       "      <th>#5000</th>\n",
       "      <th>$</th>\n",
       "      <th>%</th>\n",
       "      <th>&amp;</th>\n",
       "      <th>'</th>\n",
       "      <th>(</th>\n",
       "      <th>...</th>\n",
       "      <th>ü'll</th>\n",
       "      <th>–</th>\n",
       "      <th>—</th>\n",
       "      <th>‘</th>\n",
       "      <th>’</th>\n",
       "      <th>“</th>\n",
       "      <th>…</th>\n",
       "      <th>┾</th>\n",
       "      <th>〨ud</th>\n",
       "      <th>鈥</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 9232 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   !  \"  #  #150  #5000  $  %  &  '  ( ...  ü'll  –  —  ‘  ’  “  …  ┾  〨ud  鈥\n",
       "0  0  0  0     0      0  0  0  0  0  0 ...     0  0  0  0  0  0  0  0    0  0\n",
       "1  0  0  0     0      0  0  0  0  0  0 ...     0  0  0  0  0  0  0  0    0  0\n",
       "2  0  0  0     0      0  0  0  1  1  1 ...     0  0  0  0  0  0  0  0    0  0\n",
       "3  0  0  0     0      0  0  0  0  0  0 ...     0  0  0  0  0  0  0  0    0  0\n",
       "4  0  0  0     0      0  0  0  0  0  0 ...     0  0  0  0  0  0  0  0    0  0\n",
       "\n",
       "[5 rows x 9232 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build bag-of-words\n",
    "counter = CountVectorizer(tokenizer=casual_tokenize)\n",
    "bow_docs = pd.DataFrame(counter.fit_transform(raw_documents=sms['text']).toarray())\n",
    "column_nums, terms = zip(*sorted(zip(counter.vocabulary_.values(), counter.vocabulary_.keys())))\n",
    "bow_docs.columns = terms\n",
    "bow_docs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 9232)\n"
     ]
    }
   ],
   "source": [
    "# Create LDiA\n",
    "ldia = LDiA(n_components=num_topics, learning_method='batch', random_state=21)\n",
    "ldia.fit(bow_docs)\n",
    "\n",
    "print(ldia.components_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>topic_5</th>\n",
       "      <th>topic_6</th>\n",
       "      <th>topic_7</th>\n",
       "      <th>topic_8</th>\n",
       "      <th>topic_9</th>\n",
       "      <th>topic_10</th>\n",
       "      <th>topic_11</th>\n",
       "      <th>topic_12</th>\n",
       "      <th>topic_13</th>\n",
       "      <th>topic_14</th>\n",
       "      <th>topic_15</th>\n",
       "      <th>topic_16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>!</th>\n",
       "      <td>80.13</td>\n",
       "      <td>40.41</td>\n",
       "      <td>127.48</td>\n",
       "      <td>77.28</td>\n",
       "      <td>74.18</td>\n",
       "      <td>0.06</td>\n",
       "      <td>30.47</td>\n",
       "      <td>261.62</td>\n",
       "      <td>4.87</td>\n",
       "      <td>32.44</td>\n",
       "      <td>0.06</td>\n",
       "      <td>88.51</td>\n",
       "      <td>209.78</td>\n",
       "      <td>11.14</td>\n",
       "      <td>6.17</td>\n",
       "      <td>344.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>\"</th>\n",
       "      <td>0.06</td>\n",
       "      <td>185.50</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>26.31</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>9.10</td>\n",
       "      <td>18.21</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>6.15</td>\n",
       "      <td>9.57</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.20</td>\n",
       "      <td>3.93</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#150</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>1.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#5000</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       topic_1  topic_2  topic_3  topic_4  topic_5  topic_6  topic_7  topic_8  \\\n",
       "!        80.13    40.41   127.48    77.28    74.18     0.06    30.47   261.62   \n",
       "\"         0.06   185.50     0.06     0.06    26.31     0.06     0.06     9.10   \n",
       "#         0.06     0.06     0.20     3.93     3.06     0.06     0.06     0.06   \n",
       "#150      0.06     0.06     0.06     0.06     0.06     0.06     0.06     0.06   \n",
       "#5000     0.06     0.06     0.06     0.06     0.06     0.06     0.06     0.06   \n",
       "\n",
       "       topic_9  topic_10  topic_11  topic_12  topic_13  topic_14  topic_15  \\\n",
       "!         4.87     32.44      0.06     88.51    209.78     11.14      6.17   \n",
       "\"        18.21      0.06      0.06      6.15      9.57      0.06      0.60   \n",
       "#         0.06      0.06      0.06      0.06      0.06      0.06      0.06   \n",
       "#150      0.06      0.06      0.06      0.06      1.06      0.06      0.06   \n",
       "#5000     0.06      0.06      0.06      0.06      3.06      0.06      0.06   \n",
       "\n",
       "       topic_16  \n",
       "!        344.39  \n",
       "\"          0.06  \n",
       "#          0.06  \n",
       "#150       0.06  \n",
       "#5000      0.06  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at first five terms and their contributions to each topic\n",
    "components = pd.DataFrame(ldia.components_.T, index=terms, columns=['topic_%d' % i for i in range(1, num_topics+1)])\n",
    "components.round(2).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can look at the top values for each topic and more easily identify what each topic is trying to define. Below we look at the top 10 values and their corresponding terms for each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_1</th>\n",
       "      <th>terms_topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>terms_topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>terms_topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>terms_topic_4</th>\n",
       "      <th>topic_5</th>\n",
       "      <th>terms_topic_5</th>\n",
       "      <th>...</th>\n",
       "      <th>topic_11</th>\n",
       "      <th>terms_topic_11</th>\n",
       "      <th>topic_12</th>\n",
       "      <th>terms_topic_12</th>\n",
       "      <th>topic_13</th>\n",
       "      <th>terms_topic_13</th>\n",
       "      <th>topic_14</th>\n",
       "      <th>terms_topic_14</th>\n",
       "      <th>topic_15</th>\n",
       "      <th>terms_topic_15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>610.107651</td>\n",
       "      <td>ham</td>\n",
       "      <td>508.196283</td>\n",
       "      <td>..</td>\n",
       "      <td>372.450023</td>\n",
       "      <td>.</td>\n",
       "      <td>647.670472</td>\n",
       "      <td>.</td>\n",
       "      <td>270.128120</td>\n",
       "      <td>.</td>\n",
       "      <td>...</td>\n",
       "      <td>165.272091</td>\n",
       "      <td>.</td>\n",
       "      <td>246.787050</td>\n",
       "      <td>to</td>\n",
       "      <td>364.451456</td>\n",
       "      <td>.</td>\n",
       "      <td>34.248644</td>\n",
       "      <td>..</td>\n",
       "      <td>50.121928</td>\n",
       "      <td>your</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>389.776845</td>\n",
       "      <td>.</td>\n",
       "      <td>185.504675</td>\n",
       "      <td>\"</td>\n",
       "      <td>157.524090</td>\n",
       "      <td>a</td>\n",
       "      <td>241.846057</td>\n",
       "      <td>to</td>\n",
       "      <td>158.061485</td>\n",
       "      <td>ur</td>\n",
       "      <td>...</td>\n",
       "      <td>88.127875</td>\n",
       "      <td>:)</td>\n",
       "      <td>201.710809</td>\n",
       "      <td>a</td>\n",
       "      <td>271.265944</td>\n",
       "      <td>u</td>\n",
       "      <td>23.833487</td>\n",
       "      <td>it</td>\n",
       "      <td>35.932947</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>190.765833</td>\n",
       "      <td>the</td>\n",
       "      <td>61.861028</td>\n",
       "      <td>:-)</td>\n",
       "      <td>139.981524</td>\n",
       "      <td>i</td>\n",
       "      <td>131.298173</td>\n",
       "      <td>,</td>\n",
       "      <td>115.687288</td>\n",
       "      <td>to</td>\n",
       "      <td>...</td>\n",
       "      <td>69.581604</td>\n",
       "      <td>is</td>\n",
       "      <td>195.288309</td>\n",
       "      <td>£</td>\n",
       "      <td>209.780566</td>\n",
       "      <td>!</td>\n",
       "      <td>23.644350</td>\n",
       "      <td>to</td>\n",
       "      <td>24.599634</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>173.925769</td>\n",
       "      <td>a</td>\n",
       "      <td>59.888326</td>\n",
       "      <td>,</td>\n",
       "      <td>132.696016</td>\n",
       "      <td>you</td>\n",
       "      <td>117.045561</td>\n",
       "      <td>your</td>\n",
       "      <td>113.745415</td>\n",
       "      <td>/</td>\n",
       "      <td>...</td>\n",
       "      <td>58.810046</td>\n",
       "      <td>happy</td>\n",
       "      <td>174.020841</td>\n",
       "      <td>.</td>\n",
       "      <td>156.026120</td>\n",
       "      <td>a</td>\n",
       "      <td>23.165898</td>\n",
       "      <td>)</td>\n",
       "      <td>23.077698</td>\n",
       "      <td>for</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>159.643921</td>\n",
       "      <td>is</td>\n",
       "      <td>55.485102</td>\n",
       "      <td>u</td>\n",
       "      <td>127.484937</td>\n",
       "      <td>!</td>\n",
       "      <td>109.860679</td>\n",
       "      <td>the</td>\n",
       "      <td>81.954029</td>\n",
       "      <td>:</td>\n",
       "      <td>...</td>\n",
       "      <td>57.048932</td>\n",
       "      <td>da</td>\n",
       "      <td>102.414775</td>\n",
       "      <td>free</td>\n",
       "      <td>132.477311</td>\n",
       "      <td>you</td>\n",
       "      <td>19.051923</td>\n",
       "      <td>of</td>\n",
       "      <td>18.973849</td>\n",
       "      <td>'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>117.256129</td>\n",
       "      <td>to</td>\n",
       "      <td>50.305996</td>\n",
       "      <td>gud</td>\n",
       "      <td>117.334783</td>\n",
       "      <td>to</td>\n",
       "      <td>95.036926</td>\n",
       "      <td>on</td>\n",
       "      <td>78.532522</td>\n",
       "      <td>u</td>\n",
       "      <td>...</td>\n",
       "      <td>45.588488</td>\n",
       "      <td>..</td>\n",
       "      <td>101.540246</td>\n",
       "      <td>&amp;</td>\n",
       "      <td>113.555176</td>\n",
       "      <td>,</td>\n",
       "      <td>17.840613</td>\n",
       "      <td>$</td>\n",
       "      <td>18.165492</td>\n",
       "      <td>not</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>107.904463</td>\n",
       "      <td>spam</td>\n",
       "      <td>47.695043</td>\n",
       "      <td>k</td>\n",
       "      <td>87.741182</td>\n",
       "      <td>in</td>\n",
       "      <td>83.186471</td>\n",
       "      <td>a</td>\n",
       "      <td>76.695974</td>\n",
       "      <td>,</td>\n",
       "      <td>...</td>\n",
       "      <td>44.366254</td>\n",
       "      <td>in</td>\n",
       "      <td>97.918433</td>\n",
       "      <td>or</td>\n",
       "      <td>97.851062</td>\n",
       "      <td>to</td>\n",
       "      <td>16.957896</td>\n",
       "      <td>...</td>\n",
       "      <td>16.833176</td>\n",
       "      <td>as</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>88.560996</td>\n",
       "      <td>of</td>\n",
       "      <td>41.824031</td>\n",
       "      <td>:)</td>\n",
       "      <td>86.819730</td>\n",
       "      <td>?</td>\n",
       "      <td>77.284853</td>\n",
       "      <td>!</td>\n",
       "      <td>74.175438</td>\n",
       "      <td>!</td>\n",
       "      <td>...</td>\n",
       "      <td>34.003829</td>\n",
       "      <td>birthday</td>\n",
       "      <td>89.823160</td>\n",
       "      <td>+</td>\n",
       "      <td>92.466768</td>\n",
       "      <td>i</td>\n",
       "      <td>15.816250</td>\n",
       "      <td>the</td>\n",
       "      <td>15.987432</td>\n",
       "      <td>thanks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>85.570433</td>\n",
       "      <td>it</td>\n",
       "      <td>40.405002</td>\n",
       "      <td>!</td>\n",
       "      <td>81.695140</td>\n",
       "      <td>is</td>\n",
       "      <td>72.603963</td>\n",
       "      <td>call</td>\n",
       "      <td>55.729751</td>\n",
       "      <td>no</td>\n",
       "      <td>...</td>\n",
       "      <td>33.198496</td>\n",
       "      <td>pls</td>\n",
       "      <td>89.294104</td>\n",
       "      <td>call</td>\n",
       "      <td>90.724740</td>\n",
       "      <td>me</td>\n",
       "      <td>14.468756</td>\n",
       "      <td>that</td>\n",
       "      <td>14.062500</td>\n",
       "      <td>simple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>82.315710</td>\n",
       "      <td>he</td>\n",
       "      <td>37.393958</td>\n",
       "      <td>of</td>\n",
       "      <td>75.493247</td>\n",
       "      <td>get</td>\n",
       "      <td>61.007477</td>\n",
       "      <td>now</td>\n",
       "      <td>54.828169</td>\n",
       "      <td>for</td>\n",
       "      <td>...</td>\n",
       "      <td>28.178504</td>\n",
       "      <td>my</td>\n",
       "      <td>88.505317</td>\n",
       "      <td>!</td>\n",
       "      <td>90.142062</td>\n",
       "      <td>and</td>\n",
       "      <td>12.787564</td>\n",
       "      <td>or</td>\n",
       "      <td>13.830220</td>\n",
       "      <td>x</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      topic_1 terms_topic_1     topic_2 terms_topic_2     topic_3  \\\n",
       "0  610.107651           ham  508.196283            ..  372.450023   \n",
       "1  389.776845             .  185.504675             \"  157.524090   \n",
       "2  190.765833           the   61.861028           :-)  139.981524   \n",
       "3  173.925769             a   59.888326             ,  132.696016   \n",
       "4  159.643921            is   55.485102             u  127.484937   \n",
       "5  117.256129            to   50.305996           gud  117.334783   \n",
       "6  107.904463          spam   47.695043             k   87.741182   \n",
       "7   88.560996            of   41.824031            :)   86.819730   \n",
       "8   85.570433            it   40.405002             !   81.695140   \n",
       "9   82.315710            he   37.393958            of   75.493247   \n",
       "\n",
       "  terms_topic_3     topic_4 terms_topic_4     topic_5 terms_topic_5  \\\n",
       "0             .  647.670472             .  270.128120             .   \n",
       "1             a  241.846057            to  158.061485            ur   \n",
       "2             i  131.298173             ,  115.687288            to   \n",
       "3           you  117.045561          your  113.745415             /   \n",
       "4             !  109.860679           the   81.954029             :   \n",
       "5            to   95.036926            on   78.532522             u   \n",
       "6            in   83.186471             a   76.695974             ,   \n",
       "7             ?   77.284853             !   74.175438             !   \n",
       "8            is   72.603963          call   55.729751            no   \n",
       "9           get   61.007477           now   54.828169           for   \n",
       "\n",
       "       ...          topic_11 terms_topic_11    topic_12 terms_topic_12  \\\n",
       "0      ...        165.272091              .  246.787050             to   \n",
       "1      ...         88.127875             :)  201.710809              a   \n",
       "2      ...         69.581604             is  195.288309              £   \n",
       "3      ...         58.810046          happy  174.020841              .   \n",
       "4      ...         57.048932             da  102.414775           free   \n",
       "5      ...         45.588488             ..  101.540246              &   \n",
       "6      ...         44.366254             in   97.918433             or   \n",
       "7      ...         34.003829       birthday   89.823160              +   \n",
       "8      ...         33.198496            pls   89.294104           call   \n",
       "9      ...         28.178504             my   88.505317              !   \n",
       "\n",
       "     topic_13 terms_topic_13   topic_14 terms_topic_14   topic_15  \\\n",
       "0  364.451456              .  34.248644             ..  50.121928   \n",
       "1  271.265944              u  23.833487             it  35.932947   \n",
       "2  209.780566              !  23.644350             to  24.599634   \n",
       "3  156.026120              a  23.165898              )  23.077698   \n",
       "4  132.477311            you  19.051923             of  18.973849   \n",
       "5  113.555176              ,  17.840613              $  18.165492   \n",
       "6   97.851062             to  16.957896            ...  16.833176   \n",
       "7   92.466768              i  15.816250            the  15.987432   \n",
       "8   90.724740             me  14.468756           that  14.062500   \n",
       "9   90.142062            and  12.787564             or  13.830220   \n",
       "\n",
       "  terms_topic_15  \n",
       "0           your  \n",
       "1              .  \n",
       "2             is  \n",
       "3            for  \n",
       "4              '  \n",
       "5            not  \n",
       "6             as  \n",
       "7         thanks  \n",
       "8         simple  \n",
       "9              x  \n",
       "\n",
       "[10 rows x 30 columns]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "for i in range(1, num_topics):\n",
    "    top_vals = components['topic_' + str(i)].sort_values(ascending=False)[:10]\n",
    "    df['topic_' + str(i)] = components['topic_' + str(i)].sort_values(ascending=False)[:10].values\n",
    "    df['terms_topic_' + str(i)] = top_vals.index\n",
    "    df = df.reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above we can come up with general topic names for each topic. It looks like for topic 11 it has to do with happiness and birthdays. Topic 12 has to do with money and free things. Emails that fall into that category probably have a good chance of being spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of topic_vectors: (4837, 16)\n"
     ]
    }
   ],
   "source": [
    "# Transform BOW into topic vectors using LDiA\n",
    "topic_vectors_ldia = ldia.transform(bow_docs)\n",
    "\n",
    "print('Shape of topic_vectors:', topic_vectors_ldia.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building & Evaluating a Classifier\n",
    "---\n",
    "Once again we will use an LDA classifier to classify our text as spam or not spam and test it on custom email samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (3385, 16)\n",
      "X_test shape: (1452, 16)\n",
      "y_train shape: (3385,)\n",
      "y_test shape: (1452,)\n"
     ]
    }
   ],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(topic_vectors_ldia, sms['spam'], test_size=0.3, random_state=21)\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aaronmarquez/anaconda3/envs/nlp/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "              solver='svd', store_covariance=False, tol=0.0001)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create LDA model and fit to training data\n",
    "clf = LDA()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to get rid of collinear warning is to reduce the number of topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 91.12%\n"
     ]
    }
   ],
   "source": [
    "score = clf.score(X_test, y_test)\n",
    "\n",
    "print(\"Test accuracy: {:.2f}%\".format(100. * score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Custom Emails\n",
    "---\n",
    "We will test our dummy emails again and see how LDiA performs on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   true  preds                                              email\n",
      "0     0      0  Hi John, this is Juan. Just wanted to let you ...\n",
      "1     1      0  Earn $1000 a day. Work from home and make mone...\n",
      "2     1      0  Earn £1000 a day. Work from home and make mone...\n",
      "3     0      0  I'm going to be late, please order dinner toni...\n",
      "4     1      1  Free money!! Join our network of professionls ...\n"
     ]
    }
   ],
   "source": [
    "emails = [\"Hi John, this is Juan. Just wanted to let you know our meeting was rescheduled for next week. Will let you know more later. - Juan\",\n",
    "         \"Earn $1000 a day. Work from home and make money with me. Reply to make money now!\",\n",
    "         \"Earn £1000 a day. Work from home and make money with me. Reply to make money now!\",\n",
    "         \"I'm going to be late, please order dinner tonight for everyone.\",\n",
    "         \"Free money!! Join our network of professionls and get rewarded!\"]\n",
    "true_labels = [0, 1, 1, 0, 1]\n",
    "\n",
    "emails_bow = counter.transform(emails).toarray()\n",
    "emails_topic = ldia.transform(emails_bow)\n",
    "preds = clf.predict(emails_topic)\n",
    "test_df = pd.DataFrame()\n",
    "test_df['true'] = true_labels\n",
    "test_df['preds'] = preds\n",
    "test_df['email'] = emails\n",
    "print(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What if we try with more topics for LDiA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 91.53%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aaronmarquez/anaconda3/envs/nlp/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    }
   ],
   "source": [
    "# Create topic vectors with 32 topics\n",
    "ldia = LDiA(n_components=32, learning_method='batch', random_state=21)\n",
    "ldia.fit(bow_docs)\n",
    "topic_vectors_ldia = ldia.transform(bow_docs)\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(topic_vectors_ldia, sms['spam'], test_size=0.3, random_state=21)\n",
    "# Build & Predict\n",
    "clf = LDA()\n",
    "clf.fit(X_train, y_train)\n",
    "# Score\n",
    "score = clf.score(X_test, y_test)\n",
    "print(\"Test accuracy: {:.2f}%\".format(100. * score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "---\n",
    "LSA ended up performing better than LDiA (96.07% vs. 91.12%). To perform LSA we used PCA from sci-kit learn but we could have also used TruncatedSVD or something from the gensim library. Reducing the dimensionality of TF-IDF vectors also help prevent overfitting. The TF-IDF vectors had over 9,000 terms, far more features than samples which surely would have led to overfitting.\n",
    "\n",
    "LDiA didn't perform well on our custom emails list, missing a few spam emails. But LDiA with more topics (32 topics) performed slightly better than with 16 topics. \n",
    "\n",
    "To overcome the bias that exists in the dataset (all emails come from a British forum) we can add more data from other sources including emails that target people in other countries.\n",
    "\n",
    "The cosine similarity is a good way to find similar emails in the dataset and group them together."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
